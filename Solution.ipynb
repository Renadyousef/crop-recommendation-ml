{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5fe305c-c00b-46f1-a166-78797adf359a",
   "metadata": {},
   "source": [
    "### How Machine Learning Helps Farmers Select the Best Crops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aafe1a-ca2a-486f-aa6a-0cc618f73e92",
   "metadata": {},
   "source": [
    "<img src='farmer_in_a_field.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59164c38-ead0-44ca-9818-511c8f3d2f86",
   "metadata": {},
   "source": [
    "Measuring soil metrics like nitrogen, phosphorus, potassium, and pH is key to assessing soil health but can be costly and time-consuming. Because of budget limits, farmers often need to decide which soil properties to test.\n",
    "\n",
    "Choosing the right crop depends heavily on soil conditions, as each crop thrives under specific nutrient levels. A farmer wants help selecting the best crop based on soil data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a73c07a-6bff-421f-8c21-5de597d226c2",
   "metadata": {},
   "source": [
    "a dataset, soil_measures.csv, containing soil nutrient ratios (\"N\", \"P\", \"K\"), pH values, and the optimal crop for each field (the target). Each row shows soil measurements from a field and the crop best suited for those conditions.\n",
    "\n",
    "<h4>we will help the farmer test only the most important soil nutrient ratios for his crop data set</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b562796-ec03-44e2-a948-fd208ca64390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "crops = pd.read_csv(\"soil_measures.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aa334d6-f4cd-4f76-80dd-c9d62257a102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EDA\n",
    "crops.isnull().sum() # no missing vlaues\n",
    "crops.dtypes #all numeric ,crop is string catagorical\n",
    "len(crops['crop'].unique()) #22 class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "595f8b1f-542e-4cca-9903-cd5266639b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create features and target\n",
    "X=crops.drop('crop',axis=1)\n",
    "y=crops['crop']\n",
    "#encode our 22 classes\n",
    "# Encode target into integers since logistic cant take a matrix of one hot encoders ,only 1 dim array\n",
    "label_map = {label: idx for idx, label in enumerate(y.unique())}\n",
    "y = y.map(label_map) #1 2 3..22\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37b4aa6f-0cc3-4c6d-ba12-2dc0933592c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important is:  K 0.14300946985661483\n"
     ]
    }
   ],
   "source": [
    "# Predict the crop using each feature individually. You should build a model for each feature. That means you will #build four models.\n",
    "\n",
    "features_importance = {}\n",
    "\n",
    "for feature in [\"N\", \"P\", \"K\", \"ph\"]:\n",
    "    scaler = StandardScaler()\n",
    "    '''\n",
    "    Adjusting feature values to a standard range or distribution (e.g., mean=0, std=1 with StandardScaler)\n",
    "    to Helps models converge faster, makes features comparable.\n",
    "    Scaling when you have multiple features with different units or scales \n",
    "    \n",
    "    '''\n",
    "    log_reg = LogisticRegression(solver=\"lbfgs\", max_iter=500)  #  multi class prediction \n",
    "    \n",
    "    # Scale training data \n",
    "    # We must use the scaled data for training to improve convergence and performance warnings\n",
    "    X_train_scaled = scaler.fit_transform(X_train[[feature]]) # [[]] so its a 2 dim\n",
    "    \n",
    "    # Scale test data using the same scaler of trained data\n",
    "    # This prevents data leakage and ensures test data is transformed consistently\n",
    "    X_test_scaled = scaler.transform(X_test[[feature]])\n",
    "    \n",
    "    \n",
    "    log_reg.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    \n",
    "    y_pred = log_reg.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate the weighted F1 score for all the features\n",
    "    features_importance[feature] = metrics.f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    \n",
    "\n",
    "best_feature = max(features_importance, key=features_importance.get)\n",
    "best_f1 = features_importance[best_feature]\n",
    "\n",
    "best_predictive_feature = {best_feature: best_f1}\n",
    "\n",
    "print('Most important is: ', best_feature, best_f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
